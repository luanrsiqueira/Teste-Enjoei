{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNKVAo37oTin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c09209b-ef83-490c-dfce-43bce08a2fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5c468ae028922e92d67ed0b7f8e9bf50cd33b76beab2408811bbd436615bb96d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Instalar pySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas\n",
        "from pyspark.sql import SparkSession\n",
        "import requests\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "lOsigNlVofy_"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar sessão Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"API fakeStore\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "ATx5VXorC1Eq"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requisição dos usuários na API\n",
        "url = \"https://fakestoreapi.com/users/\"\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "df_users = spark.createDataFrame(data)"
      ],
      "metadata": {
        "id": "bT0cOgW1opxo"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requisição dos objetos que estão no carrinho\n",
        "carts = \"https://fakestoreapi.com/carts\"\n",
        "request_carts = requests.get(carts)\n",
        "response_carts = request_carts.json()\n",
        "df_carts = spark.createDataFrame(response_carts)\n",
        "\n",
        "# Função explode para quebrar a lista de dicionários da coluna 'products'\n",
        "df_carts = df_carts.withColumn(\"product\", explode(col(\"products\")))\n",
        "\n",
        "new_df_carts = df_carts.select(\n",
        "    col(\"userId\").alias(\"id_usuario\"),\n",
        "    col(\"product.productId\").alias(\"id_produto\"),\n",
        "    col(\"product.quantity\").alias(\"quantidade\"),\n",
        "    col(\"date\")\n",
        ")\n",
        "\n",
        "# Conversão data\n",
        "new_df_carts = new_df_carts.withColumn(\"data\", date_format(col(\"date\"), \"yyyy-MM-dd\"))"
      ],
      "metadata": {
        "id": "yWC7GiHRo5-Q"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requisição de todos os produtos\n",
        "products = \"https://fakestoreapi.com/products\"\n",
        "request_products = requests.get(products)\n",
        "response_products = request_products.json()\n",
        "\n",
        "# Função abaixo utilizada pois os dados estavam inconsistentes, no entanto, foi inferido o esquema mais robusto\n",
        "df_products = spark.read.json(spark.sparkContext.parallelize([response_products]))\n",
        "df_products = df_products.withColumnRenamed(\"category\", \"categoria\")"
      ],
      "metadata": {
        "id": "vsLyf4kJs25l"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join entre o dataframe \"usuário\" e \"carrinho\"\n",
        "join_df = df_users.join(new_df_carts, df_users.id == new_df_carts.id_usuario, \"inner\").select('id_usuario','id_produto','quantidade', 'data')"
      ],
      "metadata": {
        "id": "0E3W8R7eyUJt"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join entre o 'join_df' com o dataframe \"produtos\"\n",
        "join_df_2 = join_df.join(df_products, join_df.id_produto == df_products.id, \"inner\").select('id_usuario', 'id_produto', 'quantidade', 'categoria', 'data')"
      ],
      "metadata": {
        "id": "f5-5SFKXIDcV"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Somatório da quantidade de produtos e trazendo a data mais recente em cada usuário adicionou ao carrinho\n",
        "group_df = join_df_2.groupBy(\"id_usuario\", \"id_produto\", \"categoria\").agg(\n",
        "    sum(\"quantidade\").alias(\"total_quantidade\"),\n",
        "    max(\"data\").alias(\"ultima_data\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwO84U77VyLK",
        "outputId": "f76770e3-03fc-430e-cc89-b7909baa613b"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------------+----------------+-----------+\n",
            "|id_usuario|id_produto|       categoria|total_quantidade|ultima_data|\n",
            "+----------+----------+----------------+----------------+-----------+\n",
            "|         1|         1|  men's clothing|              14| 2020-03-02|\n",
            "|         1|         2|  men's clothing|               5| 2020-03-02|\n",
            "|         1|         3|  men's clothing|               6| 2020-03-02|\n",
            "|         1|         5|        jewelery|               2| 2020-01-02|\n",
            "|         2|         1|  men's clothing|               2| 2020-03-01|\n",
            "|         2|         9|     electronics|               1| 2020-03-01|\n",
            "|         3|         1|  men's clothing|               4| 2020-01-01|\n",
            "|         3|         7|        jewelery|               1| 2020-03-01|\n",
            "|         3|         8|        jewelery|               1| 2020-03-01|\n",
            "|         4|        10|     electronics|               2| 2020-03-01|\n",
            "|         4|        12|     electronics|               3| 2020-03-01|\n",
            "|         8|        18|women's clothing|               1| 2020-03-01|\n",
            "+----------+----------+----------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extração quantidade máxima de cada produto\n",
        "max_qtd_df = group_df.groupBy(\"id_usuario\").agg(\n",
        "    max(\"total_quantidade\").alias(\"max_qtd\")\n",
        ")\n",
        "max_qtd_df = max_qtd_df.withColumnRenamed(\"id_usuario\", \"id\")\n",
        "join_max_df = group_df.join(max_qtd_df, group_df.id_usuario == max_qtd_df.id, \"inner\")\n",
        "\n",
        "# Filtrando apenas a maior quantidade dos produtos de cada usuário\n",
        "final_df = join_max_df.filter(col(\"total_quantidade\") == col(\"max_qtd\")).select(\"id_usuario\", \"categoria\", \"ultima_data\").orderBy(asc(\"id_usuario\"))\n",
        "final_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfj-zRP96ZDj",
        "outputId": "602125e5-1e59-4fae-cfd9-691465245ced"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+-----------+\n",
            "|id_usuario|       categoria|ultima_data|\n",
            "+----------+----------------+-----------+\n",
            "|         1|  men's clothing| 2020-03-02|\n",
            "|         2|  men's clothing| 2020-03-01|\n",
            "|         3|  men's clothing| 2020-01-01|\n",
            "|         4|     electronics| 2020-03-01|\n",
            "|         8|women's clothing| 2020-03-01|\n",
            "+----------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar em arquivo CSV\n",
        "output_path = \"./fakeStoreSpark.csv\"\n",
        "final_df.write.option(\"header\", \"true\").csv(output_path)"
      ],
      "metadata": {
        "id": "rFyCV5YQMfo2"
      },
      "execution_count": 253,
      "outputs": []
    }
  ]
}